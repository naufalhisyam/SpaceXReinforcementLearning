{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"rocketlaunchd4pg.ipynb","provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyPywrd3Ba5US+/+bNA75FKD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"source":["gpu_info = !nvidia-smi\r\n","gpu_info = '\\n'.join(gpu_info)\r\n","if gpu_info.find('failed') >= 0:\r\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\r\n","  print('and then re-execute this cell.')\r\n","else:\r\n","  print(gpu_info)\r\n","\r\n","from psutil import virtual_memory\r\n","ram_gb = virtual_memory().total / 1e9\r\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\r\n","\r\n","if ram_gb < 20:\r\n","  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\r\n","  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\r\n","  print('re-execute this cell.')\r\n","else:\r\n","  print('You are using a high-RAM runtime!')"],"outputs":[],"metadata":{"id":"jf41_a7vlZZk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":425},"executionInfo":{"status":"ok","timestamp":1593159678397,"user_tz":-180,"elapsed":3418,"user":{"displayName":"uğurkan ateş","photoUrl":"","userId":"03432754458149960388"}},"outputId":"bdfbb6bf-f61a-498f-fec6-0761611f0fd9"}},{"cell_type":"code","execution_count":null,"source":["!pip install ptan tensorboardX box2D\r\n","!pip uninstall pyglet -y\r\n","!pip install pyglet==v1.3.2"],"outputs":[],"metadata":{"id":"qZv83CkLLEHd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":623},"executionInfo":{"status":"ok","timestamp":1593159771482,"user_tz":-180,"elapsed":87456,"user":{"displayName":"uğurkan ateş","photoUrl":"","userId":"03432754458149960388"}},"outputId":"ce9ad910-e142-4433-8ae1-9721d451baa7"}},{"cell_type":"markdown","source":["**Python Packages**"],"metadata":{"id":"yUOjoN6LLFhT","colab_type":"text"}},{"cell_type":"code","execution_count":null,"source":["!git clone https://github.com/naufalhisyam/SpaceXReinforcementLearning.git\r\n","%cd SpaceXReinforcementLearning\r\n","!ls"],"outputs":[],"metadata":{"id":"6dy1UcKPLIY7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":212},"executionInfo":{"status":"ok","timestamp":1593159780594,"user_tz":-180,"elapsed":6008,"user":{"displayName":"uğurkan ateş","photoUrl":"","userId":"03432754458149960388"}},"outputId":"35baaea3-bcf6-4d24-c069-e8f5a8ae0979"}},{"cell_type":"markdown","source":["google upload test\n"],"metadata":{"id":"bRV8nSFOblN3","colab_type":"text"}},{"cell_type":"code","execution_count":null,"source":["from google.colab import drive\r\n","drive.mount('/content/gdrive')\r\n"],"outputs":[],"metadata":{"id":"WIIwmrTkblm0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1593159784321,"user_tz":-180,"elapsed":1047,"user":{"displayName":"uğurkan ateş","photoUrl":"","userId":"03432754458149960388"}},"outputId":"cdb73c73-79f1-4dd2-dece-9d632faa40b7"}},{"cell_type":"code","execution_count":null,"source":["!apt install python-opengl\r\n","!apt install ffmpeg\r\n","!apt install xvfb\r\n","!pip3 install pyvirtualdisplay\r\n","\r\n","# Virtual display\r\n","from pyvirtualdisplay import Display\r\n","\r\n","virtual_display = Display(visible=0, size=(1400, 900))\r\n","virtual_display.start()"],"outputs":[],"metadata":{"id":"-n6umBeVPJDJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":496},"executionInfo":{"status":"ok","timestamp":1593159798251,"user_tz":-180,"elapsed":11827,"user":{"displayName":"uğurkan ateş","photoUrl":"","userId":"03432754458149960388"}},"outputId":"b0c2e8b9-b639-4b5f-bcfe-2a05b76cf4d3"}},{"cell_type":"code","execution_count":null,"source":["\r\n","\r\n","ENV_ID = \"RocketLander-v0\"\r\n","#!/usr/bin/env python3\r\n","import os\r\n","import ptan\r\n","import time\r\n","import gym\r\n","import argparse\r\n","from tensorboardX import SummaryWriter\r\n","import numpy as np\r\n","\r\n","from lib import model, common\r\n","\r\n","import torch\r\n","import torch.optim as optim\r\n","import torch.nn.functional as F\r\n","\r\n","\r\n","GAMMA = 0.99\r\n","BATCH_SIZE = 64\r\n","LEARNING_RATE = 1e-4\r\n","REPLAY_SIZE = 100000\r\n","REPLAY_INITIAL = 10000\r\n","REWARD_STEPS = 5\r\n","\r\n","TEST_ITERS = 1000\r\n","\r\n","Vmax = 10\r\n","Vmin = -10\r\n","N_ATOMS = 51\r\n","DELTA_Z = (Vmax - Vmin) / (N_ATOMS - 1)\r\n","RETURN_TRAIN = False\r\n","actor_saved_net = \"/content/gdrive/My Drive/colab_model/rocket/d4pg/run1/actor.dat\"\r\n","critic_saved_net = \"/content/gdrive/My Drive/colab_model/rocket/d4pg/run1/critic.dat\"\r\n","def test_net(net, env, count=10, device=\"cpu\"):\r\n","    rewards = 0.0\r\n","    steps = 0\r\n","    for _ in range(count):\r\n","        obs = env.reset()\r\n","        while True:\r\n","            obs_v = ptan.agent.float32_preprocessor([obs]).to(device)\r\n","            mu_v = net(obs_v)\r\n","            action = mu_v.squeeze(dim=0).data.cpu().numpy()\r\n","            action = np.clip(action, -1, 1)\r\n","            obs, reward, done, _ = env.step(action)\r\n","            rewards += reward\r\n","            steps += 1\r\n","            if done:\r\n","                break\r\n","    return rewards / count, steps / count\r\n","\r\n","\r\n","def distr_projection(next_distr_v, rewards_v, dones_mask_t,\r\n","                     gamma, device=\"cpu\"):\r\n","    next_distr = next_distr_v.data.cpu().numpy()\r\n","    rewards = rewards_v.data.cpu().numpy()\r\n","    dones_mask = dones_mask_t.cpu().numpy().astype(np.bool)\r\n","    batch_size = len(rewards)\r\n","    proj_distr = np.zeros((batch_size, N_ATOMS), dtype=np.float32)\r\n","\r\n","    for atom in range(N_ATOMS):\r\n","        tz_j = np.minimum(Vmax, np.maximum(\r\n","            Vmin, rewards + (Vmin + atom * DELTA_Z) * gamma))\r\n","        b_j = (tz_j - Vmin) / DELTA_Z\r\n","        l = np.floor(b_j).astype(np.int64)\r\n","        u = np.ceil(b_j).astype(np.int64)\r\n","        eq_mask = u == l\r\n","        proj_distr[eq_mask, l[eq_mask]] += \\\r\n","            next_distr[eq_mask, atom]\r\n","        ne_mask = u != l\r\n","        proj_distr[ne_mask, l[ne_mask]] += \\\r\n","            next_distr[ne_mask, atom] * (u - b_j)[ne_mask]\r\n","        proj_distr[ne_mask, u[ne_mask]] += \\\r\n","            next_distr[ne_mask, atom] * (b_j - l)[ne_mask]\r\n","\r\n","    if dones_mask.any():\r\n","        proj_distr[dones_mask] = 0.0\r\n","        tz_j = np.minimum(Vmax, np.maximum(\r\n","            Vmin, rewards[dones_mask]))\r\n","        b_j = (tz_j - Vmin) / DELTA_Z\r\n","        l = np.floor(b_j).astype(np.int64)\r\n","        u = np.ceil(b_j).astype(np.int64)\r\n","        eq_mask = u == l\r\n","        eq_dones = dones_mask.copy()\r\n","        eq_dones[dones_mask] = eq_mask\r\n","        if eq_dones.any():\r\n","            proj_distr[eq_dones, l[eq_mask]] = 1.0\r\n","        ne_mask = u != l\r\n","        ne_dones = dones_mask.copy()\r\n","        ne_dones[dones_mask] = ne_mask\r\n","        if ne_dones.any():\r\n","            proj_distr[ne_dones, l[ne_mask]] = (u - b_j)[ne_mask]\r\n","            proj_distr[ne_dones, u[ne_mask]] = (b_j - l)[ne_mask]\r\n","    return torch.FloatTensor(proj_distr).to(device)\r\n","\r\n","\r\n","def main():\r\n","    parser = argparse.ArgumentParser()\r\n","    name = \"test_rocket_d4pg\"\r\n","    device = torch.device(\"cuda\")\r\n","\r\n","    save_path = os.path.join(\"saves\", \"d4pg-\" + name)\r\n","    os.makedirs(save_path, exist_ok=True)\r\n","\r\n","    env = gym.make(ENV_ID)\r\n","    directory = './roketvideotest1/'\r\n","    env = gym.wrappers.Monitor(env, directory,force=True)\r\n","    test_env = gym.make(ENV_ID)\r\n","\r\n","    act_net = model.DDPGActor(env.observation_space.shape[0], env.action_space.shape[0]).to(device)\r\n","    crt_net = model.D4PGCritic(env.observation_space.shape[0], env.action_space.shape[0], N_ATOMS, Vmin, Vmax).to(device)\r\n","    if RETURN_TRAIN == True:\r\n","      act_net.load_state_dict(torch.load(args.model))\r\n","      crt_net.load_state_dict(torch.load(args.model))\r\n","\r\n","    print(act_net)\r\n","    print(crt_net)\r\n","    tgt_act_net = ptan.agent.TargetNet(act_net)\r\n","    tgt_crt_net = ptan.agent.TargetNet(crt_net)\r\n","\r\n","    writer = SummaryWriter(comment=\"-d4pg_\" + name)\r\n","    agent = model.AgentDDPG(act_net, device=device)\r\n","    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA, steps_count=REWARD_STEPS)\r\n","    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=REPLAY_SIZE)\r\n","    act_opt = optim.Adam(act_net.parameters(), lr=LEARNING_RATE)\r\n","    crt_opt = optim.Adam(crt_net.parameters(), lr=LEARNING_RATE)\r\n","\r\n","    frame_idx = 0\r\n","    best_reward = None\r\n","    with ptan.common.utils.RewardTracker(writer) as tracker:\r\n","        with ptan.common.utils.TBMeanTracker(writer, batch_size=10) as tb_tracker:\r\n","            while True:\r\n","                frame_idx += 1\r\n","                buffer.populate(1)\r\n","                rewards_steps = exp_source.pop_rewards_steps()\r\n","                if rewards_steps:\r\n","                    rewards, steps = zip(*rewards_steps)\r\n","                    tb_tracker.track(\"episode_steps\", steps[0], frame_idx)\r\n","                    tracker.reward(rewards[0], frame_idx)\r\n","\r\n","                if len(buffer) < REPLAY_INITIAL:\r\n","                    continue\r\n","\r\n","                batch = buffer.sample(BATCH_SIZE)\r\n","                states_v, actions_v, rewards_v, \\\r\n","                dones_mask, last_states_v = \\\r\n","                    common.unpack_batch_ddqn(batch, device)\r\n","\r\n","                # train critic\r\n","                crt_opt.zero_grad()\r\n","                crt_distr_v = crt_net(states_v, actions_v)\r\n","                last_act_v = tgt_act_net.target_model(\r\n","                    last_states_v)\r\n","                last_distr_v = F.softmax(\r\n","                    tgt_crt_net.target_model(\r\n","                        last_states_v, last_act_v), dim=1)\r\n","                proj_distr_v = distr_projection(\r\n","                    last_distr_v, rewards_v, dones_mask,\r\n","                    gamma=GAMMA**REWARD_STEPS, device=device)\r\n","                prob_dist_v = -F.log_softmax(\r\n","                    crt_distr_v, dim=1) * proj_distr_v\r\n","                critic_loss_v = prob_dist_v.sum(dim=1).mean()\r\n","                critic_loss_v.backward()\r\n","                crt_opt.step()\r\n","                tb_tracker.track(\"loss_critic\", critic_loss_v, frame_idx)\r\n","\r\n","                # train actor\r\n","                act_opt.zero_grad()\r\n","                cur_actions_v = act_net(states_v)\r\n","                crt_distr_v = crt_net(states_v, cur_actions_v)\r\n","                actor_loss_v = -crt_net.distr_to_q(crt_distr_v)\r\n","                actor_loss_v = actor_loss_v.mean()\r\n","                actor_loss_v.backward()\r\n","                act_opt.step()\r\n","                tb_tracker.track(\"loss_actor\", actor_loss_v,\r\n","                                 frame_idx)\r\n","\r\n","                tgt_act_net.alpha_sync(alpha=1 - 1e-3)\r\n","                tgt_crt_net.alpha_sync(alpha=1 - 1e-3)\r\n","\r\n","                if frame_idx % TEST_ITERS == 0:\r\n","                    ts = time.time()\r\n","                    rewards, steps = test_net(act_net, test_env, device=device)\r\n","                    print(\"Test done in %.2f sec, reward %.3f, steps %d\" % (\r\n","                        time.time() - ts, rewards, steps))\r\n","                    writer.add_scalar(\"test_reward\", rewards, frame_idx)\r\n","                    writer.add_scalar(\"test_steps\", steps, frame_idx)\r\n","                 \r\n","                    if best_reward is None or best_reward < rewards:\r\n","                        if best_reward is not None:\r\n","                            print(\"Best reward updated: %.3f -> %.3f\" % (best_reward, rewards))\r\n","                            name = \"best_%+.3f_%d.dat\" % (rewards, frame_idx)\r\n","                            fname_actor = os.path.join(save_path,\"actor\"+name)\r\n","                            fname_critic = os.path.join(save_path,\"critic\"+name)\r\n","                            torch.save(act_net.state_dict(),fname_actor)\r\n","                            torch.save(crt_net.state_dict,fname_critic)\r\n","                            !cp fname_actor \"/content/gdrive/My Drive/colab_model/rocket/d4pg/run1/\" # copies file to google drive\r\n","                            !cp fname_critic \"/content/gdrive/My Drive/colab_model/rocket/d4pg/run1/\" # copies file to google drive\r\n","\r\n","\r\n","                        best_reward = rewards\r\n","\r\n","    pass\r\n"],"outputs":[],"metadata":{"id":"ol2ZMBoJNfdD","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593159802184,"user_tz":-180,"elapsed":1666,"user":{"displayName":"uğurkan ateş","photoUrl":"","userId":"03432754458149960388"}}}},{"cell_type":"code","execution_count":null,"source":["main()"],"outputs":[],"metadata":{"id":"Pz8FRqzXltLK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"6b10216c-a480-4fe3-c7e4-b31c3ca93412"}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{"id":"4B25MAkWp6kT","colab_type":"code","colab":{}}}]}