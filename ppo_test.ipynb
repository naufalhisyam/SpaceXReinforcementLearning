{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Test PPO model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "#!/usr/bin/env python3\r\n",
    "import os\r\n",
    "import gym\r\n",
    "from gym import wrappers\r\n",
    "from lib import model\r\n",
    "import numpy as np\r\n",
    "import torch\r\n",
    "import ptan\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "ENV_ID = \"RocketLander-v0\"\r\n",
    "MODEL_TO_LOAD = \"D:/Coding/SpaceXReinforcementLearning/rocket_saved_network/PPO/actorbest_-3.092_1600000.dat\"\r\n",
    "RECORD_RUN = None #record dir\r\n",
    "SAVE_RUN = True #save states & actions into excel file\r\n",
    "SIMULATE_WIND = False\r\n",
    "\r\n",
    "def main():\r\n",
    "    device = torch.device(\"cpu\")\r\n",
    "    env = gym.make(ENV_ID)\r\n",
    "    if RECORD_RUN:\r\n",
    "        env = wrappers.Monitor(env, RECORD_RUN)\r\n",
    "\r\n",
    "    net = model.ModelActor(env.observation_space.shape[0], env.action_space.shape[0]).to(device)\r\n",
    "    if MODEL_TO_LOAD:\r\n",
    "        net.load_state_dict(torch.load(MODEL_TO_LOAD))\r\n",
    "\r\n",
    "    obs = env.reset()\r\n",
    "    total_reward = 0.0\r\n",
    "    total_steps = 0\r\n",
    "    left_or_right_movement = np.random.randint(0, 2)\r\n",
    "    \r\n",
    "    if SAVE_RUN:\r\n",
    "        os.makedirs(\"excel_logs\", exist_ok=True)\r\n",
    "        x_pos, y_pos, theta, eng_throttle, eng_gimbal = ([] for _ in range(5))\r\n",
    "        gimbal, throttle, side_thruster = ([] for _ in range(3))\r\n",
    "        if len(obs) == 10:\r\n",
    "            vel_x, vel_y, ang_vel = ([] for _ in range(3))\r\n",
    "    \r\n",
    "    while True:\r\n",
    "        env.render()\r\n",
    "        obs_v = ptan.agent.float32_preprocessor([obs]).to(device)\r\n",
    "        states = env.get_states_value()\r\n",
    "        mu_v = net(obs_v)[0]\r\n",
    "        mu = mu_v.squeeze(dim=0).data.cpu().numpy()\r\n",
    "        logstd = net.logstd.data.cpu().numpy()\r\n",
    "        rnd = np.random.normal(size=logstd.shape)\r\n",
    "        action = mu + np.exp(logstd) * rnd\r\n",
    "        action = np.clip(action, -1, 1)\r\n",
    "\r\n",
    "        if SAVE_RUN:\r\n",
    "            x_pos.append(states[0])\r\n",
    "            y_pos.append(states[1])\r\n",
    "            theta.append(states[2])\r\n",
    "            eng_throttle.append(states[3])\r\n",
    "            eng_gimbal.append(states[4])\r\n",
    "                \r\n",
    "            gimbal.append(action[0])\r\n",
    "            throttle.append(action[1])\r\n",
    "            side_thruster.append(action[2])\r\n",
    "                \r\n",
    "            if len(obs) == 10:\r\n",
    "                vel_x.append(states[7])\r\n",
    "                vel_y.append(states[8])\r\n",
    "                ang_vel.append(states[9])\r\n",
    "            \r\n",
    "        if np.isscalar(action): \r\n",
    "            action = [action]\r\n",
    "        obs, reward, done, _ = env.step(action)\r\n",
    "        total_reward += reward\r\n",
    "        total_steps += 1\r\n",
    "        \r\n",
    "        #Simulate wind\r\n",
    "        if SIMULATE_WIND:\r\n",
    "            if states[5] == 0 and states[6] == 0:\r\n",
    "                    env.apply_random_x_disturbance(epsilon=0.005, left_or_right=left_or_right_movement)\r\n",
    "                    env.apply_random_y_disturbance(epsilon=0.005)\r\n",
    "            \r\n",
    "        if done:\r\n",
    "            if SAVE_RUN:\r\n",
    "                if len(obs) == 10:\r\n",
    "                    state_dat=pd.DataFrame(list(zip(x_pos, y_pos, theta, eng_throttle, eng_gimbal, vel_x, vel_y, ang_vel)),\\\r\n",
    "                        columns=['x_pos', 'y_pos', 'theta', 'engine_throttle', 'engine_gimbal', 'vel_x', 'vel_y', 'ang_vel'])\r\n",
    "                else:\r\n",
    "                    state_dat=pd.DataFrame(list(zip(x_pos, y_pos, theta, eng_throttle, eng_gimbal)),\\\r\n",
    "                        columns=['x_pos', 'y_pos', 'theta', 'engine_throttle', 'engine_gimbal'])\r\n",
    "                    \r\n",
    "                act_dat=pd.DataFrame(list(zip(gimbal, throttle, side_thruster)),columns=['gimbal', 'throttle', 'side_thruster'])\r\n",
    "                with pd.ExcelWriter(f\"excel_logs\\ppo_{total_reward}_{total_steps}.xlsx\") as writer:\r\n",
    "                    state_dat.to_excel(writer, sheet_name=\"state\")\r\n",
    "                    act_dat.to_excel(writer, sheet_name=\"action\")\r\n",
    "                \r\n",
    "            env.render(close=True)\r\n",
    "            break\r\n",
    "    print(\"In %d steps we got %.3f reward\" % (total_steps, total_reward))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "main()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('pitorch': conda)"
  },
  "interpreter": {
   "hash": "b0345f6219c9aae003de93abf944447c5305266c0b8600cfe1450bd72b59a316"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}