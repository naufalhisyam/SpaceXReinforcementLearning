{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "Run PPO on local runtime"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "import os\r\n",
                "import math\r\n",
                "import ptan\r\n",
                "import time\r\n",
                "import gym\r\n",
                "import argparse\r\n",
                "from tensorboardX import SummaryWriter\r\n",
                "\r\n",
                "from lib import model, test_net, calc_logprob\r\n",
                "\r\n",
                "import numpy as np\r\n",
                "import torch\r\n",
                "import torch.optim as optim\r\n",
                "import torch.nn.functional as F\r\n",
                "\r\n",
                "RETURN_TRAIN = False\r\n",
                "\r\n",
                "actor_saved_net = \"D:/Coding/SpaceXReinforcementLearning/rocket_saved_network/PPO/actorbest_-3.092_1600000.dat\"\r\n",
                "critic_saved_net = \"D:/Coding/SpaceXReinforcementLearning/rocket_saved_network/PPO/criticbest_-3.092_1600000.dat\"\r\n",
                "\r\n",
                "ENV_ID = \"RocketLander-v0\"\r\n",
                "GAMMA = 0.99\r\n",
                "GAE_LAMBDA = 0.95\r\n",
                "\r\n",
                "TRAJECTORY_SIZE = 4097\r\n",
                "LEARNING_RATE_ACTOR = 1e-5\r\n",
                "LEARNING_RATE_CRITIC = 1e-4\r\n",
                "\r\n",
                "PPO_EPS = 0.2\r\n",
                "PPO_EPOCHES = 10\r\n",
                "PPO_BATCH_SIZE = 256\r\n",
                "\r\n",
                "TEST_ITERS = 100000\r\n",
                "test_count = 0\r\n",
                "\r\n",
                "def calc_adv_ref(trajectory, net_crt, states_v, device=\"cpu\"):\r\n",
                "    \"\"\"\r\n",
                "    By trajectory calculate advantage and 1-step ref value\r\n",
                "    :param trajectory: trajectory list\r\n",
                "    :param net_crt: critic network\r\n",
                "    :param states_v: states tensor\r\n",
                "    :return: tuple with advantage numpy array and reference values\r\n",
                "    \"\"\"\r\n",
                "    values_v = net_crt(states_v)\r\n",
                "    values = values_v.squeeze().data.cpu().numpy()\r\n",
                "    # generalized advantage estimator: smoothed version of the advantage\r\n",
                "    last_gae = 0.0\r\n",
                "    result_adv = []\r\n",
                "    result_ref = []\r\n",
                "    for val, next_val, (exp,) in zip(reversed(values[:-1]),\r\n",
                "                                     reversed(values[1:]),\r\n",
                "                                     reversed(trajectory[:-1])):\r\n",
                "        if exp.done:\r\n",
                "            delta = exp.reward - val\r\n",
                "            last_gae = delta\r\n",
                "        else:\r\n",
                "            delta = exp.reward + GAMMA * next_val - val\r\n",
                "            last_gae = delta + GAMMA * GAE_LAMBDA * last_gae\r\n",
                "        result_adv.append(last_gae)\r\n",
                "        result_ref.append(last_gae + val)\r\n",
                "\r\n",
                "    adv_v = torch.FloatTensor(list(reversed(result_adv)))\r\n",
                "    ref_v = torch.FloatTensor(list(reversed(result_ref)))\r\n",
                "    return adv_v.to(device), ref_v.to(device)\r\n",
                "\r\n",
                "\r\n",
                "def main():\r\n",
                "    test_count = 0\r\n",
                "    name = \"ppo_run2\" #Make sure to change this to avoid file overwriting\r\n",
                "    device = torch.device(\"cpu\")\r\n",
                "\r\n",
                "    save_path = os.path.join(\"saves\", \"ppo-\" + name)\r\n",
                "    os.makedirs(save_path, exist_ok=True)\r\n",
                "\r\n",
                "    env = gym.make(ENV_ID)\r\n",
                "    directory = './roketvideotest2/' #Make sure to change this to avoid file overwriting\r\n",
                "    env = gym.wrappers.Monitor(env, directory,force=True)\r\n",
                "    test_env = gym.make(ENV_ID)\r\n",
                "\r\n",
                "    net_act = model.ModelActor(env.observation_space.shape[0], env.action_space.shape[0]).to(device)\r\n",
                "    net_crt = model.ModelCritic(env.observation_space.shape[0]).to(device)\r\n",
                "    if RETURN_TRAIN == True:\r\n",
                "      net_act.load_state_dict(torch.load(actor_saved_net))\r\n",
                "      net_crt.load_state_dict(torch.load(critic_saved_net))\r\n",
                "    \r\n",
                "    print(net_act)\r\n",
                "    print(net_crt)\r\n",
                "\r\n",
                "    writer = SummaryWriter(comment=\"-ppo_\" + name)\r\n",
                "    agent = model.AgentA2C(net_act, device=device)\r\n",
                "    exp_source = ptan.experience.ExperienceSource(env, agent, steps_count=1)\r\n",
                "\r\n",
                "    opt_act = optim.Adam(net_act.parameters(), lr=LEARNING_RATE_ACTOR)\r\n",
                "    opt_crt = optim.Adam(net_crt.parameters(), lr=LEARNING_RATE_CRITIC)\r\n",
                "\r\n",
                "    trajectory = []\r\n",
                "    best_reward = None\r\n",
                "    with ptan.common.utils.RewardTracker(writer) as tracker:\r\n",
                "        for step_idx, exp in enumerate(exp_source):\r\n",
                "            rewards_steps = exp_source.pop_rewards_steps()\r\n",
                "            if rewards_steps:\r\n",
                "                rewards, steps = zip(*rewards_steps)\r\n",
                "                writer.add_scalar(\"episode_steps\", np.mean(steps), step_idx)\r\n",
                "                tracker.reward(np.mean(rewards), step_idx)\r\n",
                "\r\n",
                "            if step_idx % TEST_ITERS == 0:\r\n",
                "                if(test_count == 15):\r\n",
                "                  print(\"resetting best_wards\")\r\n",
                "                  best_reward = None\r\n",
                "                test_count += 1\r\n",
                "                ts = time.time()\r\n",
                "                rewards, steps = test_net(net_act, test_env, device=device)\r\n",
                "                print(\"Test done in %.2f sec, reward %.3f, steps %d\" % (\r\n",
                "                    time.time() - ts, rewards, steps))\r\n",
                "                writer.add_scalar(\"test_reward\", rewards, step_idx)\r\n",
                "                writer.add_scalar(\"test_steps\", steps, step_idx)\r\n",
                "                if best_reward is None or best_reward < rewards:\r\n",
                "                    if best_reward is not None:\r\n",
                "                        print(\"Best reward updated: %.3f -> %.3f\" % (best_reward, rewards))\r\n",
                "                        name = \"best_%+.3f_%d.dat\" % (rewards, step_idx)\r\n",
                "                        fname_actor = os.path.join(save_path,\"actor\"+name)\r\n",
                "                        fname_critic = os.path.join(save_path,\"critic\"+name)\r\n",
                "                        torch.save(net_act.state_dict(), fname_actor)\r\n",
                "                        torch.save(net_crt.state_dict(),fname_critic)\r\n",
                "                        #!cp fname_actor \"/content/gdrive/My Drive/colab_model/rocket/ppo/run1/\" # copies file to google drive\r\n",
                "                        #!cp fname_critic \"/content/gdrive/My Drive/colab_model/rocket/ppo/run1/\" # copies file to google drive\r\n",
                "                    best_reward = rewards\r\n",
                "\r\n",
                "            trajectory.append(exp)\r\n",
                "            if len(trajectory) < TRAJECTORY_SIZE:\r\n",
                "                continue\r\n",
                "\r\n",
                "            traj_states = [t[0].state for t in trajectory]\r\n",
                "            traj_actions = [t[0].action for t in trajectory]\r\n",
                "            traj_states_v = torch.FloatTensor(traj_states)\r\n",
                "            traj_states_v = traj_states_v.to(device)\r\n",
                "            traj_actions_v = torch.FloatTensor(traj_actions)\r\n",
                "            traj_actions_v = traj_actions_v.to(device)\r\n",
                "            traj_adv_v, traj_ref_v = calc_adv_ref(\r\n",
                "                trajectory, net_crt, traj_states_v, device=device)\r\n",
                "            mu_v = net_act(traj_states_v)\r\n",
                "            old_logprob_v = calc_logprob(\r\n",
                "                mu_v, net_act.logstd, traj_actions_v)\r\n",
                "\r\n",
                "            # normalize advantages\r\n",
                "            traj_adv_v = traj_adv_v - torch.mean(traj_adv_v)\r\n",
                "            traj_adv_v /= torch.std(traj_adv_v)\r\n",
                "\r\n",
                "            # drop last entry from the trajectory, an our adv and ref value calculated without it\r\n",
                "            trajectory = trajectory[:-1]\r\n",
                "            old_logprob_v = old_logprob_v[:-1].detach()\r\n",
                "\r\n",
                "            sum_loss_value = 0.0\r\n",
                "            sum_loss_policy = 0.0\r\n",
                "            count_steps = 0\r\n",
                "\r\n",
                "            for epoch in range(PPO_EPOCHES):\r\n",
                "                for batch_ofs in range(0, len(trajectory),\r\n",
                "                                       PPO_BATCH_SIZE):\r\n",
                "                    batch_l = batch_ofs + PPO_BATCH_SIZE\r\n",
                "                    states_v = traj_states_v[batch_ofs:batch_l]\r\n",
                "                    actions_v = traj_actions_v[batch_ofs:batch_l]\r\n",
                "                    batch_adv_v = traj_adv_v[batch_ofs:batch_l]\r\n",
                "                    batch_adv_v = batch_adv_v.unsqueeze(-1)\r\n",
                "                    batch_ref_v = traj_ref_v[batch_ofs:batch_l]\r\n",
                "                    batch_old_logprob_v = \\\r\n",
                "                        old_logprob_v[batch_ofs:batch_l]\r\n",
                "\r\n",
                "                    # critic training\r\n",
                "                    opt_crt.zero_grad()\r\n",
                "                    value_v = net_crt(states_v)\r\n",
                "                    loss_value_v = F.mse_loss(\r\n",
                "                        value_v.squeeze(-1), batch_ref_v)\r\n",
                "                    loss_value_v.backward()\r\n",
                "                    opt_crt.step()\r\n",
                "\r\n",
                "                    # actor training\r\n",
                "                    opt_act.zero_grad()\r\n",
                "                    mu_v = net_act(states_v)\r\n",
                "                    logprob_pi_v = calc_logprob(\r\n",
                "                        mu_v, net_act.logstd, actions_v)\r\n",
                "                    ratio_v = torch.exp(\r\n",
                "                        logprob_pi_v - batch_old_logprob_v)\r\n",
                "                    surr_obj_v = batch_adv_v * ratio_v\r\n",
                "                    c_ratio_v = torch.clamp(ratio_v,\r\n",
                "                                            1.0 - PPO_EPS,\r\n",
                "                                            1.0 + PPO_EPS)\r\n",
                "                    clipped_surr_v = batch_adv_v * c_ratio_v\r\n",
                "                    loss_policy_v = -torch.min(\r\n",
                "                        surr_obj_v, clipped_surr_v).mean()\r\n",
                "                    loss_policy_v.backward()\r\n",
                "                    opt_act.step()\r\n",
                "\r\n",
                "                    sum_loss_value += loss_value_v.item()\r\n",
                "                    sum_loss_policy += loss_policy_v.item()\r\n",
                "                    count_steps += 1\r\n",
                "\r\n",
                "            trajectory.clear()\r\n",
                "            writer.add_scalar(\"advantage\", traj_adv_v.mean().item(), step_idx)\r\n",
                "            writer.add_scalar(\"values\", traj_ref_v.mean().item(), step_idx)\r\n",
                "            writer.add_scalar(\"loss_policy\", sum_loss_policy / count_steps, step_idx)\r\n",
                "            writer.add_scalar(\"loss_value\", sum_loss_value / count_steps, step_idx)\r\n",
                "\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "%load_ext tensorboard"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "logs_base_dir = \"./logs\"\r\n",
                "os.makedirs(logs_base_dir, exist_ok=True)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "main()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "%tensorboard --logdir {logs_base_dir}"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.7.11",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.7.11 64-bit ('pitorch': conda)"
        },
        "interpreter": {
            "hash": "b0345f6219c9aae003de93abf944447c5305266c0b8600cfe1450bd72b59a316"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}