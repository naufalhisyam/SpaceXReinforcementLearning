{"cells":[{"cell_type":"code","execution_count":null,"source":["gpu_info = !nvidia-smi\r\n","gpu_info = '\\n'.join(gpu_info)\r\n","if gpu_info.find('failed') >= 0:\r\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\r\n","  print('and then re-execute this cell.')\r\n","else:\r\n","  print(gpu_info)\r\n","\r\n","from psutil import virtual_memory\r\n","ram_gb = virtual_memory().total / 1e9\r\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\r\n","\r\n","if ram_gb < 20:\r\n","  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\r\n","  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\r\n","  print('re-execute this cell.')\r\n","else:\r\n","  print('You are using a high-RAM runtime!')"],"outputs":[],"metadata":{"id":"jf41_a7vlZZk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":425},"executionInfo":{"status":"ok","timestamp":1593268353711,"user_tz":-180,"elapsed":2527,"user":{"displayName":"uğurkan ateş","photoUrl":"","userId":"03432754458149960388"}},"outputId":"db9accd5-cc56-4cb4-e6bd-68e8a80efc73"}},{"cell_type":"code","execution_count":null,"source":["!pip install ptan tensorboardX box2D box2D-py\r\n","!pip uninstall gym -y\r\n","!pip uninstall pyglet -y\r\n","!pip install pyglet==v1.3.2"],"outputs":[],"metadata":{"id":"qZv83CkLLEHd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":587},"executionInfo":{"status":"ok","timestamp":1593268364366,"user_tz":-180,"elapsed":12259,"user":{"displayName":"uğurkan ateş","photoUrl":"","userId":"03432754458149960388"}},"outputId":"81386f35-cb70-445c-abb0-4e1d80d1023f"}},{"cell_type":"code","execution_count":null,"source":["!git clone https://github.com/naufalhisyam/SpaceXReinforcementLearning.git\r\n","%cd SpaceXReinforcementLearning\r\n","!ls"],"outputs":[],"metadata":{"id":"6dy1UcKPLIY7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":158},"executionInfo":{"status":"ok","timestamp":1593268368126,"user_tz":-180,"elapsed":13851,"user":{"displayName":"uğurkan ateş","photoUrl":"","userId":"03432754458149960388"}},"outputId":"e9138a49-7999-458e-a566-e76b5d9600ec"}},{"cell_type":"code","execution_count":null,"source":["from google.colab import drive\r\n","drive.mount('/content/gdrive')"],"outputs":[],"metadata":{"id":"WIIwmrTkblm0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1593268368129,"user_tz":-180,"elapsed":11615,"user":{"displayName":"uğurkan ateş","photoUrl":"","userId":"03432754458149960388"}},"outputId":"5a2b36f5-fd2f-4d7d-b356-4e473d650feb"}},{"cell_type":"code","execution_count":null,"source":["!apt install python-opengl\r\n","!apt install ffmpeg\r\n","!apt install xvfb\r\n","!pip3 install pyvirtualdisplay\r\n","\r\n","# Virtual display\r\n","from pyvirtualdisplay import Display\r\n","\r\n","virtual_display = Display(visible=0, size=(1400, 900))\r\n","virtual_display.start()"],"outputs":[],"metadata":{"id":"-n6umBeVPJDJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":496},"executionInfo":{"status":"ok","timestamp":1593268390542,"user_tz":-180,"elapsed":18842,"user":{"displayName":"uğurkan ateş","photoUrl":"","userId":"03432754458149960388"}},"outputId":"574ad03c-16da-4096-8da1-09534dd9233a"}},{"cell_type":"code","execution_count":null,"source":["#!/usr/bin/env python3\r\n","import os\r\n","import math\r\n","import ptan\r\n","import time\r\n","import gym\r\n","import argparse\r\n","from tensorboardX import SummaryWriter\r\n","\r\n","from lib import model, test_net, calc_logprob\r\n","\r\n","import numpy as np\r\n","import torch\r\n","import torch.optim as optim\r\n","import torch.nn.functional as F\r\n","\r\n","RETURN_TRAIN = False\r\n","# You have to manually change this for your drive , folders has to exists in your Drive.\r\n","actor_saved_net = \"/content/gdrive/My Drive/colab_model/rocket/ppo/run1/actor.dat\"\r\n","critic_saved_net = \"/content/gdrive/My Drive/colab_model/rocket/ppo/run1/critic.dat\"\r\n","\r\n","ENV_ID = \"RocketLander-v0\"\r\n","GAMMA = 0.99\r\n","GAE_LAMBDA = 0.95\r\n","\r\n","TRAJECTORY_SIZE = 4097\r\n","LEARNING_RATE_ACTOR = 1e-5\r\n","LEARNING_RATE_CRITIC = 1e-4\r\n","\r\n","PPO_EPS = 0.2\r\n","PPO_EPOCHES = 10\r\n","PPO_BATCH_SIZE = 256\r\n","\r\n","TEST_ITERS = 100000\r\n","test_count = 0\r\n","\r\n","def calc_adv_ref(trajectory, net_crt, states_v, device=\"cpu\"):\r\n","    \"\"\"\r\n","    By trajectory calculate advantage and 1-step ref value\r\n","    :param trajectory: trajectory list\r\n","    :param net_crt: critic network\r\n","    :param states_v: states tensor\r\n","    :return: tuple with advantage numpy array and reference values\r\n","    \"\"\"\r\n","    values_v = net_crt(states_v)\r\n","    values = values_v.squeeze().data.cpu().numpy()\r\n","    # generalized advantage estimator: smoothed version of the advantage\r\n","    last_gae = 0.0\r\n","    result_adv = []\r\n","    result_ref = []\r\n","    for val, next_val, (exp,) in zip(reversed(values[:-1]),\r\n","                                     reversed(values[1:]),\r\n","                                     reversed(trajectory[:-1])):\r\n","        if exp.done:\r\n","            delta = exp.reward - val\r\n","            last_gae = delta\r\n","        else:\r\n","            delta = exp.reward + GAMMA * next_val - val\r\n","            last_gae = delta + GAMMA * GAE_LAMBDA * last_gae\r\n","        result_adv.append(last_gae)\r\n","        result_ref.append(last_gae + val)\r\n","\r\n","    adv_v = torch.FloatTensor(list(reversed(result_adv)))\r\n","    ref_v = torch.FloatTensor(list(reversed(result_ref)))\r\n","    return adv_v.to(device), ref_v.to(device)\r\n","\r\n","\r\n","def main():\r\n","    test_count = 0\r\n","    name = \"pporun1\"\r\n","    device = torch.device(\"cuda\")\r\n","\r\n","    save_path = os.path.join(\"saves\", \"ppo-\" + name)\r\n","    os.makedirs(save_path, exist_ok=True)\r\n","\r\n","    env = gym.make(ENV_ID)\r\n","    directory = './roketvideotest2/'\r\n","    env = gym.wrappers.Monitor(env, directory,force=True)\r\n","    test_env = gym.make(ENV_ID)\r\n","\r\n","    net_act = model.ModelActor(env.observation_space.shape[0], env.action_space.shape[0]).to(device)\r\n","    net_crt = model.ModelCritic(env.observation_space.shape[0]).to(device)\r\n","    if RETURN_TRAIN == True:\r\n","        net_act.load_state_dict(torch.load(actor_saved_net))\r\n","        net_crt.load_state_dict(torch.load(critic_saved_net))\r\n","    print(net_act)\r\n","    print(net_crt)\r\n","\r\n","    writer = SummaryWriter(comment=\"-ppo_\" + name)\r\n","    agent = model.AgentA2C(net_act, device=device)\r\n","    exp_source = ptan.experience.ExperienceSource(env, agent, steps_count=1)\r\n","\r\n","    opt_act = optim.Adam(net_act.parameters(), lr=LEARNING_RATE_ACTOR)\r\n","    opt_crt = optim.Adam(net_crt.parameters(), lr=LEARNING_RATE_CRITIC)\r\n","\r\n","    trajectory = []\r\n","    best_reward = None\r\n","    with ptan.common.utils.RewardTracker(writer) as tracker:\r\n","        for step_idx, exp in enumerate(exp_source):\r\n","            rewards_steps = exp_source.pop_rewards_steps()\r\n","            if rewards_steps:\r\n","                rewards, steps = zip(*rewards_steps)\r\n","                writer.add_scalar(\"episode_steps\", np.mean(steps), step_idx)\r\n","                tracker.reward(np.mean(rewards), step_idx)\r\n","\r\n","            if step_idx % TEST_ITERS == 0:\r\n","                if(test_count == 15):\r\n","                  print(\"resetting best_wards\")\r\n","                  best_reward = None\r\n","                test_count += 1\r\n","                ts = time.time()\r\n","                rewards, steps = test_net(net_act, test_env, device=device)\r\n","                print(\"Test done in %.2f sec, reward %.3f, steps %d\" % (\r\n","                    time.time() - ts, rewards, steps))\r\n","                writer.add_scalar(\"test_reward\", rewards, step_idx)\r\n","                writer.add_scalar(\"test_steps\", steps, step_idx)\r\n","                if best_reward is None or best_reward < rewards:\r\n","                    if best_reward is not None:\r\n","                        print(\"Best reward updated: %.3f -> %.3f\" % (best_reward, rewards))\r\n","                        name = \"best_%+.3f_%d.dat\" % (rewards, step_idx)\r\n","                        fname_actor = os.path.join(save_path,\"actor\"+name)\r\n","                        fname_critic = os.path.join(save_path,\"critic\"+name)\r\n","                        torch.save(net_act.state_dict(), fname_actor)\r\n","                        torch.save(net_crt.state_dict(),fname_critic)\r\n","                        !cp fname_actor \"/content/gdrive/My Drive/colab_model/rocket/ppo/run1/\" # copies file to google drive\r\n","                        !cp fname_critic \"/content/gdrive/My Drive/colab_model/rocket/ppo/run1/\" # copies file to google drive\r\n","                    best_reward = rewards\r\n","\r\n","            trajectory.append(exp)\r\n","            if len(trajectory) < TRAJECTORY_SIZE:\r\n","                continue\r\n","\r\n","            traj_states = [t[0].state for t in trajectory]\r\n","            traj_actions = [t[0].action for t in trajectory]\r\n","            traj_states_v = torch.FloatTensor(traj_states)\r\n","            traj_states_v = traj_states_v.to(device)\r\n","            traj_actions_v = torch.FloatTensor(traj_actions)\r\n","            traj_actions_v = traj_actions_v.to(device)\r\n","            traj_adv_v, traj_ref_v = calc_adv_ref(\r\n","                trajectory, net_crt, traj_states_v, device=device)\r\n","            mu_v = net_act(traj_states_v)\r\n","            old_logprob_v = calc_logprob(\r\n","                mu_v, net_act.logstd, traj_actions_v)\r\n","\r\n","            # normalize advantages\r\n","            traj_adv_v = traj_adv_v - torch.mean(traj_adv_v)\r\n","            traj_adv_v /= torch.std(traj_adv_v)\r\n","\r\n","            # drop last entry from the trajectory, an our adv and ref value calculated without it\r\n","            trajectory = trajectory[:-1]\r\n","            old_logprob_v = old_logprob_v[:-1].detach()\r\n","\r\n","            sum_loss_value = 0.0\r\n","            sum_loss_policy = 0.0\r\n","            count_steps = 0\r\n","\r\n","            for epoch in range(PPO_EPOCHES):\r\n","                for batch_ofs in range(0, len(trajectory),\r\n","                                       PPO_BATCH_SIZE):\r\n","                    batch_l = batch_ofs + PPO_BATCH_SIZE\r\n","                    states_v = traj_states_v[batch_ofs:batch_l]\r\n","                    actions_v = traj_actions_v[batch_ofs:batch_l]\r\n","                    batch_adv_v = traj_adv_v[batch_ofs:batch_l]\r\n","                    batch_adv_v = batch_adv_v.unsqueeze(-1)\r\n","                    batch_ref_v = traj_ref_v[batch_ofs:batch_l]\r\n","                    batch_old_logprob_v = \\\r\n","                        old_logprob_v[batch_ofs:batch_l]\r\n","\r\n","                    # critic training\r\n","                    opt_crt.zero_grad()\r\n","                    value_v = net_crt(states_v)\r\n","                    loss_value_v = F.mse_loss(\r\n","                        value_v.squeeze(-1), batch_ref_v)\r\n","                    loss_value_v.backward()\r\n","                    opt_crt.step()\r\n","\r\n","                    # actor training\r\n","                    opt_act.zero_grad()\r\n","                    mu_v = net_act(states_v)\r\n","                    logprob_pi_v = calc_logprob(\r\n","                        mu_v, net_act.logstd, actions_v)\r\n","                    ratio_v = torch.exp(\r\n","                        logprob_pi_v - batch_old_logprob_v)\r\n","                    surr_obj_v = batch_adv_v * ratio_v\r\n","                    c_ratio_v = torch.clamp(ratio_v,\r\n","                                            1.0 - PPO_EPS,\r\n","                                            1.0 + PPO_EPS)\r\n","                    clipped_surr_v = batch_adv_v * c_ratio_v\r\n","                    loss_policy_v = -torch.min(\r\n","                        surr_obj_v, clipped_surr_v).mean()\r\n","                    loss_policy_v.backward()\r\n","                    opt_act.step()\r\n","\r\n","                    sum_loss_value += loss_value_v.item()\r\n","                    sum_loss_policy += loss_policy_v.item()\r\n","                    count_steps += 1\r\n","\r\n","            trajectory.clear()\r\n","            writer.add_scalar(\"advantage\", traj_adv_v.mean().item(), step_idx)\r\n","            writer.add_scalar(\"values\", traj_ref_v.mean().item(), step_idx)\r\n","            writer.add_scalar(\"loss_policy\", sum_loss_policy / count_steps, step_idx)\r\n","            writer.add_scalar(\"loss_value\", sum_loss_value / count_steps, step_idx)"],"outputs":[],"metadata":{"id":"ol2ZMBoJNfdD","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593268426895,"user_tz":-180,"elapsed":1027,"user":{"displayName":"uğurkan ateş","photoUrl":"","userId":"03432754458149960388"}}}},{"cell_type":"code","execution_count":null,"source":["main()"],"outputs":[],"metadata":{"id":"Pz8FRqzXltLK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":370},"executionInfo":{"status":"error","timestamp":1593276488779,"user_tz":-180,"elapsed":9872,"user":{"displayName":"uğurkan ateş","photoUrl":"","userId":"03432754458149960388"}},"outputId":"c5c0d54b-4eb7-4c7e-9581-03b59fecead1"}}],"metadata":{"colab":{"name":"rocketppo.ipynb","provenance":[{"file_id":"1U93rCtj22EyciL29RdspBKLIWK5YLGz4","timestamp":1593266621829}],"toc_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyPXdOnbcprmSOedIICGohtk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":2}